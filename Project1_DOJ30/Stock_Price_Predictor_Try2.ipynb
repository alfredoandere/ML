{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Stock Price Predictor\n",
    "By: Alfredo Andere Valencia <br>\n",
    "<br>\n",
    "Used data from the S&P 500 stock market and a Deep Neural Network (DNN) with features:\n",
    "<i>\n",
    "<br> x1-x30    : Opening Price of stock for past 30 days, \n",
    "<br> x31-x60   : Highest price of the day of past 30 days, \n",
    "<br> x61-x90   : Lowest price of the day for past 30 days, \n",
    "<br> x91-x120  : Closing price of past 30 days, \n",
    "<br> x121-x150 : Volume of trades of past 30 days.\n",
    "</i>\n",
    "<br>\n",
    "<br> The DNN tries to learn and predict:\n",
    "<i>\n",
    "<br> [1, 0] : Stocks will go up in price in the next 24 hours, \n",
    "<br> [0, 1]  : stocks price will decrease in price in the next 24 hours <br> </i>\n",
    "<br>\n",
    "<b>Dataset:</b> <br>\n",
    "<i>DJIA 30 Stock Time Series</i> <br>\n",
    "Historical stock data for DIJA 30 companies (2006-01-01 to 2018-01-01): <br>\n",
    "https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231 <br>\n",
    "(Date, Open, High, Low, Close, Volume, Name)\n",
    "\n",
    "<i>S&P 500 stock data </i> <br>\n",
    "Historical stock data for all current S&P 500 companies (2013 - 2018) <br>\n",
    "https://www.kaggle.com/camnugent/sandp500 <br>\n",
    "(date, open, high, low, close, volume, Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alfredoanderejr/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date    Open    High     Low   Close   Volume Name\n",
      "0  2017-01-03  178.83  180.00  177.22  178.05  2510055  MMM\n",
      "1  2017-01-04  178.03  178.90  177.61  178.32  1541985  MMM\n",
      "2  2017-01-05  178.26  179.14  176.89  177.71  1447848  MMM\n",
      "3  2017-01-06  177.29  178.60  175.80  178.23  1625049  MMM\n",
      "4  2017-01-09  178.37  178.38  177.20  177.27  1622625  MMM\n",
      "         date   Open   High    Low  Close    Volume Name\n",
      "0  2013-02-08  15.07  15.12  14.63  14.75   8407500  AAL\n",
      "1  2013-02-11  14.89  15.01  14.26  14.46   8882000  AAL\n",
      "2  2013-02-12  14.45  14.51  14.10  14.27   8126000  AAL\n",
      "3  2013-02-13  14.30  14.94  14.25  14.66  10259500  AAL\n",
      "4  2013-02-14  14.94  14.96  13.16  13.99  31879900  AAL\n"
     ]
    }
   ],
   "source": [
    "# Load in data into a pandas dataframe\n",
    "stocks_DOJ30 = pd.read_csv('stock-time-series-20050101-to-20171231/all_stocks_2017-01-01_to_2018-01-01.csv')\n",
    "stocks_500 = pd.read_csv('all_stocks_5yr.csv')\n",
    "\n",
    "# Capitalize columns to match first dataset\n",
    "stocks_500 = stocks_500.rename(columns={'open':'Open', 'high':'High', 'low':'Low', 'close':'Close', 'volume':'Volume'})\n",
    "\n",
    "\n",
    "print(stocks_DOJ30.head()) \n",
    "print(stocks_500.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150580</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.150967</td>\n",
       "      <td>0.151086</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.149906</td>\n",
       "      <td>0.149581</td>\n",
       "      <td>0.151299</td>\n",
       "      <td>0.151315</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.149781</td>\n",
       "      <td>0.150686</td>\n",
       "      <td>0.150797</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.149283</td>\n",
       "      <td>0.149330</td>\n",
       "      <td>0.149757</td>\n",
       "      <td>0.151238</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.149146</td>\n",
       "      <td>0.150950</td>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close    Volume Name\n",
       "0  0.150580  0.150500  0.150967  0.151086  0.008033  MMM\n",
       "1  0.149906  0.149581  0.151299  0.151315  0.004935  MMM\n",
       "2  0.150100  0.149781  0.150686  0.150797  0.004634  MMM\n",
       "3  0.149283  0.149330  0.149757  0.151238  0.005201  MMM\n",
       "4  0.150192  0.149146  0.150950  0.150424  0.005193  MMM"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the mean of each column and subtract it from column, \n",
    "# take the range of  each column and subtract it from column\n",
    "def mean_normalize(dataframe):\n",
    "    minus_mean = (dataframe - dataframe.mean()) \n",
    "    df_range = (dataframe.max() - dataframe.min())\n",
    "    df_norm = dataframe / df_range\n",
    "    return df_norm\n",
    "    \n",
    "normalized_DOJ30 = mean_normalize(stocks_DOJ30[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "normalized_500 = mean_normalize(stocks_500[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "\n",
    "normalized_DOJ30 = pd.concat([normalized_DOJ30, stocks_DOJ30['Name']], axis=1)\n",
    "normalized_500 = pd.concat([normalized_500, stocks_500['Name']], axis=1)\n",
    "\n",
    "all_stocks_normalized = pd.concat([normalized_DOJ30, normalized_500], axis=0, ignore_index=True)\n",
    "\n",
    "all_stocks_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_stocks_normalized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0059411bff62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mall_stocks_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_stocks_normalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_stocks_normalized' is not defined"
     ]
    }
   ],
   "source": [
    "stock_names = []\n",
    "\n",
    "# Create featuresets out of data\n",
    "def data_handling(dataframe):\n",
    "    dataset = np.array([])\n",
    "    i, j = 0, 30 * 6\n",
    "    for _ in range(dataframe['Name'].count()):\n",
    "        try:\n",
    "            current_name = dataframe.loc[j]['Name']\n",
    "            stock_names.append(current_name)\n",
    "            while dataframe.loc[j+1]['Name'] == current_name:\n",
    "                    temp = dataframe[i:j]\n",
    "                    feature = (pd.concat([temp['Open'], temp['High'], temp['Low'], temp['Close'], temp['Volume']], axis=0))\n",
    "                    feature = feature.values.transpose()\n",
    "                    feature = np.array(feature)\n",
    "                    label = [1, 0] if (dataframe.loc[j+1]['Open'] - dataframe.loc[j]['Open'] > 0) else [0, 1]\n",
    "                    feature_label = np.array([[feature, label]])\n",
    "                    dataset = np.concatenate([dataset, [feature, label]], axis=0)\n",
    "                    i += 7\n",
    "                    j += 7\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "        i, j = j + 1, j + (30 * 6)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "all_stocks_matrix = data_handling(all_stocks_normalized)\n",
    "print(stock_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Organize Data into Features and Labels\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "i = 0\n",
    "while i < len(all_stocks_matrix):\n",
    "    X.append(all_stocks_matrix[i])\n",
    "    i += 1\n",
    "    Y.append(all_stocks_matrix[i])\n",
    "    i += 1\n",
    "\n",
    "# Remove any NaN in data and replace it by priors day value\n",
    "for feature in X:\n",
    "    index = 0\n",
    "    for i in feature:\n",
    "        if (math.isnan(i)):\n",
    "            feature[index] = feature[index-1]\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "610740"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the order of data \n",
    "\n",
    "Data = list(zip(X, Y))\n",
    "random.shuffle(Data)\n",
    "X, Y = zip(*Data)\n",
    "\n",
    "\n",
    "print(len(X) == len(Y))                \n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate into training and testing set\n",
    "test_size = 0.2\n",
    "\n",
    "train_x = X[0:-int(len(X)*test_size)]\n",
    "train_y = Y[0:-int(len(X)*test_size)]\n",
    "test_x = X[int(-len(X)*test_size):]\n",
    "test_y = Y[int(-len(X)*test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 1000\n",
    "n_nodes_hl2 = 1000\n",
    "n_nodes_hl3 = 1000\n",
    "n_classes = 2\n",
    "batch_size = 5000\n",
    "epoch_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN with three hidden layers\n",
    "def model(data):\n",
    "    hidden_layer_1 = {'weights':tf.Variable(tf.random_normal([len(train_x[1]), n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    hidden_layer_2 = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_layer_3 = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    output = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal(([n_classes])))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data, hidden_layer_1['weights']), hidden_layer_1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_layer_2['weights']), hidden_layer_2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_layer_3['weights']), hidden_layer_3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.matmul(l3, output['weights']) + output['biases']\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Session of tensorflow to train DNN\n",
    "def train_net(x):\n",
    "    prediction = model(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    n_epochs = epoch_num\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "                epoch_loss += c\n",
    "                i += batch_size\n",
    "                \n",
    "                \n",
    "            print('epoch:', epoch, 'completed out of:', n_epochs, 'with loss:', epoch_loss)\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "        print('Accuracy:', accuracy.eval({x: test_x, y: test_y}))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-cd099d254b96>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "epoch: 0 completed out of: 100 with loss: 105168.39735412598\n",
      "epoch: 1 completed out of: 100 with loss: 38404.261947631836\n",
      "epoch: 2 completed out of: 100 with loss: 57151.72896575928\n",
      "epoch: 3 completed out of: 100 with loss: 30212.614051818848\n",
      "epoch: 4 completed out of: 100 with loss: 53558.54050445557\n",
      "epoch: 5 completed out of: 100 with loss: 40603.30177307129\n",
      "epoch: 6 completed out of: 100 with loss: 40816.25161743164\n",
      "epoch: 7 completed out of: 100 with loss: 44250.83182525635\n",
      "epoch: 8 completed out of: 100 with loss: 32992.33726501465\n",
      "epoch: 9 completed out of: 100 with loss: 56562.81783294678\n",
      "epoch: 10 completed out of: 100 with loss: 45918.434257507324\n",
      "epoch: 11 completed out of: 100 with loss: 45847.39234542847\n",
      "epoch: 12 completed out of: 100 with loss: 30838.07448577881\n",
      "epoch: 13 completed out of: 100 with loss: 44836.686264038086\n",
      "epoch: 14 completed out of: 100 with loss: 34847.401988983154\n",
      "epoch: 15 completed out of: 100 with loss: 24539.242694854736\n",
      "epoch: 16 completed out of: 100 with loss: 44650.455936431885\n",
      "epoch: 17 completed out of: 100 with loss: 26155.222286224365\n",
      "epoch: 18 completed out of: 100 with loss: 31331.84600830078\n",
      "epoch: 19 completed out of: 100 with loss: 38732.12427520752\n",
      "epoch: 20 completed out of: 100 with loss: 30964.67264175415\n",
      "epoch: 21 completed out of: 100 with loss: 22306.024921417236\n",
      "epoch: 22 completed out of: 100 with loss: 35744.34650039673\n",
      "epoch: 23 completed out of: 100 with loss: 21670.37300491333\n",
      "epoch: 24 completed out of: 100 with loss: 35911.38597869873\n",
      "epoch: 25 completed out of: 100 with loss: 28822.597499847412\n",
      "epoch: 26 completed out of: 100 with loss: 29910.656810760498\n",
      "epoch: 27 completed out of: 100 with loss: 43492.94332122803\n",
      "epoch: 28 completed out of: 100 with loss: 28508.972091674805\n",
      "epoch: 29 completed out of: 100 with loss: 24567.61278152466\n",
      "epoch: 30 completed out of: 100 with loss: 26082.00559616089\n",
      "epoch: 31 completed out of: 100 with loss: 36310.06760406494\n",
      "epoch: 32 completed out of: 100 with loss: 27511.899673461914\n",
      "epoch: 33 completed out of: 100 with loss: 22634.990421295166\n",
      "epoch: 34 completed out of: 100 with loss: 16427.400157928467\n",
      "epoch: 35 completed out of: 100 with loss: 16130.06350326538\n",
      "epoch: 36 completed out of: 100 with loss: 19738.56862258911\n",
      "epoch: 37 completed out of: 100 with loss: 21498.339324951172\n",
      "epoch: 38 completed out of: 100 with loss: 20823.755889892578\n",
      "epoch: 39 completed out of: 100 with loss: 21356.108951568604\n",
      "epoch: 40 completed out of: 100 with loss: 19139.800804138184\n",
      "epoch: 41 completed out of: 100 with loss: 20361.021797180176\n",
      "epoch: 42 completed out of: 100 with loss: 17853.487552642822\n",
      "epoch: 43 completed out of: 100 with loss: 19538.346981048584\n",
      "epoch: 44 completed out of: 100 with loss: 24989.096160888672\n",
      "epoch: 45 completed out of: 100 with loss: 30048.389450073242\n",
      "epoch: 46 completed out of: 100 with loss: 14785.006172180176\n",
      "epoch: 47 completed out of: 100 with loss: 25402.107849121094\n",
      "epoch: 48 completed out of: 100 with loss: 18218.663875579834\n",
      "epoch: 49 completed out of: 100 with loss: 14843.839054107666\n",
      "epoch: 50 completed out of: 100 with loss: 16375.276765823364\n",
      "epoch: 51 completed out of: 100 with loss: 20702.933923721313\n",
      "epoch: 52 completed out of: 100 with loss: 14996.689693450928\n",
      "epoch: 53 completed out of: 100 with loss: 22421.547355651855\n",
      "epoch: 54 completed out of: 100 with loss: 20109.216426849365\n",
      "epoch: 55 completed out of: 100 with loss: 14727.015686035156\n",
      "epoch: 56 completed out of: 100 with loss: 10376.95170211792\n",
      "epoch: 57 completed out of: 100 with loss: 15808.291095733643\n",
      "epoch: 58 completed out of: 100 with loss: 19477.054941177368\n",
      "epoch: 59 completed out of: 100 with loss: 20406.722091674805\n",
      "epoch: 60 completed out of: 100 with loss: 14302.928470611572\n",
      "epoch: 61 completed out of: 100 with loss: 12793.314405441284\n",
      "epoch: 62 completed out of: 100 with loss: 21239.52236175537\n",
      "epoch: 63 completed out of: 100 with loss: 12416.924089431763\n",
      "epoch: 64 completed out of: 100 with loss: 14889.968872070312\n",
      "epoch: 65 completed out of: 100 with loss: 16515.21999359131\n",
      "epoch: 66 completed out of: 100 with loss: 19542.08814048767\n",
      "epoch: 67 completed out of: 100 with loss: 16483.042093276978\n",
      "epoch: 68 completed out of: 100 with loss: 21342.71429824829\n",
      "epoch: 69 completed out of: 100 with loss: 13132.531816482544\n",
      "epoch: 70 completed out of: 100 with loss: 8898.530317306519\n",
      "epoch: 71 completed out of: 100 with loss: 9629.19246673584\n",
      "epoch: 72 completed out of: 100 with loss: 11624.005823135376\n",
      "epoch: 73 completed out of: 100 with loss: 9299.300954818726\n",
      "epoch: 74 completed out of: 100 with loss: 19879.121604919434\n",
      "epoch: 75 completed out of: 100 with loss: 15613.648008346558\n",
      "epoch: 76 completed out of: 100 with loss: 8606.13486289978\n",
      "epoch: 77 completed out of: 100 with loss: 8603.932136535645\n",
      "epoch: 78 completed out of: 100 with loss: 7374.523807525635\n",
      "epoch: 79 completed out of: 100 with loss: 10118.797979354858\n",
      "epoch: 80 completed out of: 100 with loss: 9406.346685409546\n",
      "epoch: 81 completed out of: 100 with loss: 17311.425895690918\n",
      "epoch: 82 completed out of: 100 with loss: 12142.9063205719\n",
      "epoch: 83 completed out of: 100 with loss: 14546.213809967041\n",
      "epoch: 84 completed out of: 100 with loss: 10095.556045532227\n",
      "epoch: 85 completed out of: 100 with loss: 8129.773601531982\n",
      "epoch: 86 completed out of: 100 with loss: 6597.407772064209\n",
      "epoch: 87 completed out of: 100 with loss: 10006.880392074585\n",
      "epoch: 88 completed out of: 100 with loss: 13614.224203109741\n",
      "epoch: 89 completed out of: 100 with loss: 7364.014600753784\n",
      "epoch: 90 completed out of: 100 with loss: 7676.874984741211\n",
      "epoch: 91 completed out of: 100 with loss: 10661.253620147705\n",
      "epoch: 92 completed out of: 100 with loss: 9843.14842414856\n",
      "epoch: 93 completed out of: 100 with loss: 6916.112363815308\n",
      "epoch: 94 completed out of: 100 with loss: 8878.86010169983\n",
      "epoch: 95 completed out of: 100 with loss: 10256.481546401978\n",
      "epoch: 96 completed out of: 100 with loss: 8456.514331817627\n",
      "epoch: 97 completed out of: 100 with loss: 7774.246570587158\n",
      "epoch: 98 completed out of: 100 with loss: 5520.601406097412\n",
      "epoch: 99 completed out of: 100 with loss: 6759.680417060852\n",
      "Accuracy: 0.52228445\n"
     ]
    }
   ],
   "source": [
    "train_net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
